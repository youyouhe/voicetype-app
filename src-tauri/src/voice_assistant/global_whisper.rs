use std::sync::Arc;
use tokio::sync::RwLock;
use serde::{Serialize, Deserialize};
use std::sync::OnceLock;

use crate::voice_assistant::asr::whisper_rs::{WhisperRSProcessor, WhisperRSConfig};
use crate::voice_assistant::traits::VoiceError;

/// å…¨å±€WhisperRSå®ä¾‹ç®¡ç†å™¨
pub struct GlobalWhisperManager {
    processor: Option<Arc<std::sync::Mutex<WhisperRSProcessor>>>,
    current_model_path: Option<String>,
    init_in_progress: bool,
}

impl GlobalWhisperManager {
    /// åˆ›å»ºæ–°çš„ç®¡ç†å™¨å®ä¾‹
    pub fn new() -> Self {
        Self {
            processor: None,
            current_model_path: None,
            init_in_progress: false,
        }
    }

    /// è·å–æˆ–åˆ›å»ºWhisperRSå¤„ç†å™¨
    pub async fn get_or_create_processor(&mut self, model_path: &str) -> Result<Arc<std::sync::Mutex<WhisperRSProcessor>>, VoiceError> {
        // æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç›¸åŒæ¨¡å‹çš„å¤„ç†å™¨
        if let Some(current_path) = &self.current_model_path {
            if current_path == model_path {
                if let Some(processor) = &self.processor {
                    println!("âœ… Reusing existing WhisperRS processor for model: {}", model_path);
                    return Ok(Arc::clone(processor));
                }
            }
        }

        // å¦‚æœæ­£åœ¨åˆå§‹åŒ–ï¼Œç­‰å¾…å®Œæˆ
        if self.init_in_progress {
            println!("â³ WhisperRS processor initialization in progress, waiting...");
            // è¿™é‡Œå¯ä»¥æ·»åŠ ç­‰å¾…é€»è¾‘ï¼Œä½†ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç›´æ¥è¿”å›é”™è¯¯
            return Err(VoiceError::Other("WhisperRS processor initialization in progress".to_string()));
        }

        // éœ€è¦åˆ›å»ºæ–°çš„å¤„ç†å™¨
        println!("ğŸ”§ Initializing new WhisperRS processor for model: {}", model_path);
        self.init_in_progress = true;

        // Auto-detect optimal GPU backend
        let gpu_detector = crate::voice_assistant::asr::gpu_detector::GpuDetector::new();
        let optimal_backend = gpu_detector.get_preferred_backend();

        let config = WhisperRSConfig {
            model_path: model_path.to_string(),
            language: None, // Auto-detect
            sampling_strategy: crate::voice_assistant::asr::whisper_rs::SamplingStrategyConfig::Greedy { best_of: 1 },
            translate: false,
            enable_vad: std::env::var("WHISPER_ENABLE_VAD")
                .unwrap_or_else(|_| "false".to_string())
                .parse::<bool>()
                .unwrap_or(false),
            backend: optimal_backend.clone(),
            use_gpu_if_available: std::env::var("WHISPER_USE_GPU")
                .unwrap_or_else(|_| "true".to_string())
                .parse::<bool>()
                .unwrap_or(true),
            gpu_device_id: std::env::var("WHISPER_GPU_DEVICE_ID")
                .ok()
                .and_then(|id| id.parse::<u32>().ok()),
        };

        match WhisperRSProcessor::new(config) {
            Ok(processor) => {
                let arc_processor = Arc::new(std::sync::Mutex::new(processor));
                self.processor = Some(Arc::clone(&arc_processor));
                self.current_model_path = Some(model_path.to_string());
                self.init_in_progress = false;

                // è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¿æŒå…¼å®¹æ€§
                std::env::set_var("WHISPER_MODEL_PATH", model_path);

                println!("âœ… WhisperRS processor initialized successfully for model: {}", model_path);
                Ok(arc_processor)
            }
            Err(e) => {
                self.init_in_progress = false;
                println!("âŒ Failed to initialize WhisperRS processor: {}", e);
                Err(e)
            }
        }
    }

    /// æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å¤„ç†å™¨
    pub fn has_processor(&self) -> bool {
        self.processor.is_some()
    }

    /// è·å–å½“å‰æ¨¡å‹è·¯å¾„
    pub fn get_current_model_path(&self) -> Option<&str> {
        self.current_model_path.as_deref()
    }

    /// æ¸…é™¤å½“å‰å¤„ç†å™¨ï¼ˆç”¨äºé”™è¯¯æ¢å¤æˆ–æ¨¡å‹å¸è½½ï¼‰
    pub fn clear_processor(&mut self) {
        println!("ğŸ—‘ï¸ Clearing global WhisperRS processor");
        self.processor = None;
        self.current_model_path = None;
        self.init_in_progress = false;
    }

    /// å¼ºåˆ¶é‡æ–°åŠ è½½å¤„ç†å™¨
    pub async fn force_reload(&mut self, model_path: &str) -> Result<Arc<std::sync::Mutex<WhisperRSProcessor>>, VoiceError> {
        println!("ğŸ”„ Force reloading WhisperRS processor for model: {}", model_path);
        self.clear_processor();
        self.get_or_create_processor(model_path).await
    }
}

/// å…¨å±€WhisperRSç®¡ç†å™¨å®ä¾‹
static GLOBAL_WHISPER_MANAGER: OnceLock<RwLock<GlobalWhisperManager>> = OnceLock::new();

/// è·å–å…¨å±€WhisperRSç®¡ç†å™¨å®ä¾‹
pub fn get_global_whisper_manager() -> &'static RwLock<GlobalWhisperManager> {
    GLOBAL_WHISPER_MANAGER.get_or_init(|| RwLock::new(GlobalWhisperManager::new()))
}

/// ä¾¿åˆ©å‡½æ•°ï¼šè·å–æˆ–åˆ›å»ºWhisperRSå¤„ç†å™¨
pub async fn get_or_create_whisper_processor(model_path: &str) -> Result<Arc<std::sync::Mutex<WhisperRSProcessor>>, VoiceError> {
    let manager = get_global_whisper_manager();
    let mut manager_guard = manager.write().await;
    manager_guard.get_or_create_processor(model_path).await
}

/// ä¾¿åˆ©å‡½æ•°ï¼šå¼ºåˆ¶é‡æ–°åŠ è½½å¤„ç†å™¨
pub async fn force_reload_whisper_processor(model_path: &str) -> Result<Arc<std::sync::Mutex<WhisperRSProcessor>>, VoiceError> {
    let manager = get_global_whisper_manager();
    let mut manager_guard = manager.write().await;
    manager_guard.force_reload(model_path).await
}

/// ä¾¿åˆ©å‡½æ•°ï¼šæ¸…é™¤å…¨å±€å¤„ç†å™¨
pub async fn clear_global_whisper_processor() {
    let manager = get_global_whisper_manager();
    let mut manager_guard = manager.write().await;
    manager_guard.clear_processor();
}

/// æ£€æŸ¥å…¨å±€å¤„ç†å™¨çŠ¶æ€
pub async fn get_global_whisper_status() -> serde_json::Value {
    let manager = get_global_whisper_manager();
    let manager_guard = manager.read().await;
    
    serde_json::json!({
        "has_processor": manager_guard.has_processor(),
        "current_model_path": manager_guard.get_current_model_path(),
        "init_in_progress": false // ç”±äºå‡½æ•°ä½œç”¨åŸŸé™åˆ¶ï¼Œè¿™é‡Œè¿”å›å›ºå®šå€¼
    })
}

#[derive(Debug, Serialize, Deserialize)]
pub struct WhisperManagerStatus {
    pub has_processor: bool,
    pub current_model_path: Option<String>,
    pub init_in_progress: bool,
}

/// Tauriå‘½ä»¤ï¼šè·å–å…¨å±€WhisperRSçŠ¶æ€
#[tauri::command]
pub async fn get_whisper_manager_status() -> Result<WhisperManagerStatus, String> {
    let status = get_global_whisper_status().await;
    serde_json::from_value(status).map_err(|e| format!("Failed to serialize status: {}", e))
}

/// Tauriå‘½ä»¤ï¼šå¼ºåˆ¶é‡æ–°åŠ è½½WhisperRSå¤„ç†å™¨
#[tauri::command]
pub async fn reload_whisper_processor(model_path: String) -> Result<String, String> {
    match force_reload_whisper_processor(&model_path).await {
        Ok(_) => {
            println!("âœ… WhisperRS processor reloaded successfully");
            Ok(format!("Successfully reloaded WhisperRS processor for model: {}", model_path))
        }
        Err(e) => {
            println!("âŒ Failed to reload WhisperRS processor: {}", e);
            Err(format!("Failed to reload WhisperRS processor: {}", e))
        }
    }
}

/// Tauriå‘½ä»¤ï¼šæ¸…é™¤å…¨å±€WhisperRSå¤„ç†å™¨
#[tauri::command]
pub async fn clear_whisper_processor() -> Result<String, String> {
    clear_global_whisper_processor().await;
    println!("âœ… Global WhisperRS processor cleared");
    Ok("Global WhisperRS processor cleared successfully".to_string())
}